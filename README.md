# Image Captioning using LSTM and CNN
## Description of the Problem
The Image Captioning project aims to generate captions for images automatically. The captions are generated by a deep learning model, which combines Convolutional Neural Networks (CNNs) for image processing and Long Short-Term Memory (LSTM) networks for sequence processing.

### Dataset
The dataset used in this project is Flicker8k. It contains around 8000 images with corresponding captions.
![image_captioning_dataset](https://user-images.githubusercontent.com/74714252/228909136-9c5ee7a7-1281-41ee-af50-210913231f76.png)


### Dependencies
Python 3.6 or higher
TensorFlow 2.0 or higher
Keras 2.0 or higher
NumPy
Pillow

## Data Preprocessing
The data preprocessing involves the following steps:

- Load the image dataset
- Preprocess the images using CNNs
- Preprocess the captions using LSTM

### Load the image dataset
The first step is to load the image dataset from Flicker8k. The images are resized to 224x224 pixels, and the pixel values are normalized to be in the range of [0, 1].

**Preprocess the images using CNNs**

The images are processed using a pre-trained CNN model (VGG-16) to extract features. The model is fine-tuned on the image captioning task by removing the last layer and adding a new layer to output the feature vector.

**Preprocess the captions using LSTM**

The captions are preprocessed using LSTM networks. The words are tokenized, and a vocabulary is created. The captions are padded to have the same length, and the word indices are converted to one-hot vectors.

## Model Architecture
The model architecture consists of a CNN for image processing and an LSTM for sequence processing. The image features are fed into the LSTM along with the word embeddings of the caption. The output of the LSTM is a sequence of words that form the caption.
![model](https://user-images.githubusercontent.com/74714252/228908085-319930a6-9251-4fb5-a7c5-181a1404869d.png)


## Evaluation Metric
The evaluation metric used in this project is BLEU. BLEU (Bilingual Evaluation Understudy) is a metric for evaluating the quality of generated text. It compares the generated text to a set of reference texts and computes a score based on the n-grams overlap.

## Usage
To train the model, run the train.py script. The script takes the following arguments:

--epochs: the number of epochs to train the model (default: 20)
--batch_size: the batch size for training (default: 32)
To generate captions for new images, run the predict.py script. The script takes the following arguments:

--image_path: the path to the image to generate the caption for
## Conclusion
In this project, we have implemented an image captioning model using LSTM and CNN. The model has been trained on the Flicker8k dataset and evaluated using the BLEU metric. The trained model can generate captions for new images, making it a useful tool for automatic image annotation.
